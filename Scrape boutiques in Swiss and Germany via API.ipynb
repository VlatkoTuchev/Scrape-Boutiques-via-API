{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5b1d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credentials import credentials\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2e908f",
   "metadata": {},
   "outputs": [],
   "source": [
    "centers_switzerland = [\n",
    "    \"Zurich, Switzerland\",\n",
    "    \"Geneva, Switzerland\",\n",
    "    \"Basel, Switzerland\",\n",
    "    \"Lausanne, Switzerland\",\n",
    "    \"Bern, Switzerland\",\n",
    "    \"Winterthur, Switzerland\",\n",
    "    \"Lucerne, Switzerland\",\n",
    "    \"St. Gallen, Switzerland\",\n",
    "    \"Lugano, Switzerland\",\n",
    "    \"Biel/Bienne, Switzerland\",\n",
    "    \"Thun, Switzerland\",\n",
    "    \"Köniz, Switzerland\",\n",
    "    \"La Chaux-de-Fonds, Switzerland\",\n",
    "    \"Schaffhausen, Switzerland\",\n",
    "    \"Fribourg, Switzerland\",\n",
    "    \"Chur, Switzerland\",\n",
    "    \"Neuchâtel, Switzerland\",\n",
    "    \"Uster, Switzerland\",\n",
    "    \"Sion, Switzerland\"\n",
    "]\n",
    "\n",
    "centers_germany = [\n",
    "    \"Berlin, Germany\",\n",
    "    \"Hamburg, Germany\",\n",
    "    \"Munich, Germany\",\n",
    "    \"Cologne, Germany\",\n",
    "    \"Frankfurt, Germany\",\n",
    "    \"Stuttgart, Germany\",\n",
    "    \"Düsseldorf, Germany\",\n",
    "    \"Dortmund, Germany\",\n",
    "    \"Essen, Germany\",\n",
    "    \"Leipzig, Germany\",\n",
    "    \"Bremen, Germany\",\n",
    "    \"Dresden, Germany\",\n",
    "    \"Hanover, Germany\",\n",
    "    \"Nuremberg, Germany\",\n",
    "    \"Duisburg, Germany\",\n",
    "    \"Bochum, Germany\",\n",
    "    \"Wuppertal, Germany\",\n",
    "    \"Bielefeld, Germany\",\n",
    "    \"Bonn, Germany\",\n",
    "    \"Münster, Germany\",\n",
    "    \"Karlsruhe, Germany\",\n",
    "    \"Mannheim, Germany\",\n",
    "    \"Augsburg, Germany\",\n",
    "    \"Wiesbaden, Germany\",\n",
    "    \"Gelsenkirchen, Germany\",\n",
    "    \"Mönchengladbach, Germany\",\n",
    "    \"Braunschweig, Germany\",\n",
    "    \"Chemnitz, Germany\",\n",
    "    \"Kiel, Germany\",\n",
    "    \"Aachen, Germany\",\n",
    "    \"Halle, Germany\",\n",
    "    \"Magdeburg, Germany\",\n",
    "    \"Freiburg, Germany\",\n",
    "    \"Krefeld, Germany\",\n",
    "    \"Lübeck, Germany\",\n",
    "    \"Oberhausen, Germany\",\n",
    "    \"Erfurt, Germany\",\n",
    "    \"Mainz, Germany\",\n",
    "    \"Rostock, Germany\",\n",
    "    \"Kassel, Germany\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3850713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Maps API Key\n",
    "API_KEY = credentials['api_key']\n",
    "\n",
    "# Google Places and Place Details endpoint URLs\n",
    "BASE_PLACES_URL = 'https://maps.googleapis.com/maps/api/place/nearbysearch/json'\n",
    "PLACE_DETAILS_URL = 'https://maps.googleapis.com/maps/api/place/details/json'\n",
    "\n",
    "\n",
    "def get_coordinates_for_city(city_name):\n",
    "    \"\"\"Fetch the latitude and longitude for a given city using Google's Geocoding API.\"\"\"\n",
    "    print(city_name)\n",
    "    \n",
    "    params = {\n",
    "        'address': city_name,\n",
    "        'key': API_KEY\n",
    "    }\n",
    "    \n",
    "    response = requests.get(BASE_GEOCODING_URL, params=params)\n",
    "    location_data = response.json()\n",
    "    print(location_data)\n",
    "    location = location_data['results'][0]['geometry']['location']\n",
    "    \n",
    "    return location['lat'], location['lng']\n",
    "\n",
    "\n",
    "def get_place_details(place_id):\n",
    "    \"\"\"Retrieve detailed information for a given place using its place_id.\"\"\"\n",
    "    \n",
    "    params = {\n",
    "        'place_id': place_id,\n",
    "        'fields': 'name,address_component,formatted_phone_number,website',\n",
    "        'key': API_KEY\n",
    "    }\n",
    "    \n",
    "    response = requests.get(PLACE_DETAILS_URL, params=params)\n",
    "    \n",
    "    return response.json().get('result', {})\n",
    "\n",
    "\n",
    "def get_boutiques_for_center(lat, lng):\n",
    "    \"\"\"Fetch boutiques near a given latitude and longitude.\"\"\"\n",
    "    \n",
    "    boutiques = []\n",
    "    params = {\n",
    "        'location': f'{lat},{lng}',\n",
    "        'radius': 15000,\n",
    "        'type': 'store',\n",
    "        'keyword': 'boutique',\n",
    "        'key': API_KEY\n",
    "    }\n",
    "    \n",
    "    response = requests.get(BASE_PLACES_URL, params=params)\n",
    "    results = response.json()\n",
    "    \n",
    "    boutiques.extend(results.get('results', []))\n",
    "    \n",
    "    # Get additional results from next pages (if they exist)\n",
    "    while 'next_page_token' in results:\n",
    "        time.sleep(2)  # Wait a bit before next request to ensure the token is valid\n",
    "        params['pagetoken'] = results['next_page_token']\n",
    "        response = requests.get(BASE_PLACES_URL, params=params)\n",
    "        results = response.json()\n",
    "        boutiques.extend(results.get('results', []))\n",
    "    \n",
    "    return boutiques\n",
    "\n",
    "\n",
    "def process_centers(centers):\n",
    "    \"\"\"Retrieve boutiques details for a list of city centers.\"\"\"\n",
    "    \n",
    "    all_boutiques = []\n",
    "    \n",
    "    for center in centers:\n",
    "        lat, lng = get_coordinates_for_city(center)\n",
    "        boutiques = get_boutiques_for_center(lat, lng)\n",
    "        \n",
    "        for boutique in boutiques:\n",
    "            details = get_place_details(boutique['place_id'])\n",
    "\n",
    "            # Extracting required information\n",
    "            name = details.get('name', '')\n",
    "            address = ''.join([component['long_name'] + ' ' for component in details.get('address_components', []) if component['types'][0] in ['route', 'street_number']]).strip()\n",
    "            postal_code = ''.join([component['long_name'] for component in details.get('address_components', []) if 'postal_code' in component['types']])\n",
    "            phone_number = details.get('formatted_phone_number', '')\n",
    "            website = details.get('website', '')\n",
    "\n",
    "            all_boutiques.append({\n",
    "                'Name': name,\n",
    "                'Address': address,\n",
    "                'PLZ': postal_code,\n",
    "                'Phone Number': phone_number,\n",
    "                'Website': website\n",
    "            })\n",
    "\n",
    "    return all_boutiques\n",
    "\n",
    "\n",
    "# Process Switzerland and Germany centers separately and save results to Excel\n",
    "df_switzerland = pd.DataFrame(process_centers(centers_switzerland)).drop_duplicates()\n",
    "df_switzerland.to_excel('swiss_boutiques.xlsx', index=False)\n",
    "\n",
    "df_germany = pd.DataFrame(process_centers(centers_germany)).drop_duplicates()\n",
    "df_germany.to_excel('german_boutiques.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74a9ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_swiss_boutiques = pd.read_excel('boutiques_in_switzerland.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70411974",
   "metadata": {},
   "outputs": [],
   "source": [
    "websites = data_swiss_boutiques['Website']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f55ddda",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fetch emails from all of the websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae10028d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import threading\n",
    "\n",
    "BASE_URLS = websites[len(emails_list):]\n",
    "\n",
    "class EmailFoundException(Exception):\n",
    "    \"\"\"Exception raised when an email is found.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def is_relevant_link(base_url, href):\n",
    "    \"\"\"Determine if the link is relevant for our search.\"\"\"\n",
    "\n",
    "    # Exclude specific file types\n",
    "    ignored_extensions = ['.jpg', '.jpeg', '.png', '.css', '.js', '.gif', '.svg', '.webp']\n",
    "    if any(href.endswith(ext) for ext in ignored_extensions):\n",
    "        return False\n",
    "\n",
    "    # Limit to same domain\n",
    "    parsed_base = urlparse(base_url)\n",
    "    parsed_href = urlparse(href)\n",
    "    if parsed_href.netloc and parsed_base.netloc != parsed_href.netloc:\n",
    "        return False\n",
    "\n",
    "    # Exclude certain URL patterns\n",
    "    ignored_patterns = [\"/api/\", \"/assets/\"]\n",
    "    if any(pattern in href for pattern in ignored_patterns):\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def find_emails_on_page(base_url, url, visited_urls, depth=0, max_depth=3):\n",
    "    # If we've visited this URL before, skip it\n",
    "    if url in visited_urls:\n",
    "        return None\n",
    "\n",
    "    # Add the URL to our set of visited URLs\n",
    "    visited_urls.add(url)\n",
    "\n",
    "    try:\n",
    "        # Fetch the content of the page\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # raise exception for bad responses\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Extract emails with regex\n",
    "        email_pattern = re.compile(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,3}\")\n",
    "        page_emails = email_pattern.findall(soup.text)\n",
    "        if page_emails:\n",
    "            raise EmailFoundException(page_emails[0])  # raise exception with the found email\n",
    "\n",
    "        # Limit the depth of the recursion to avoid getting stuck in loops\n",
    "        if depth > max_depth:\n",
    "            return None\n",
    "\n",
    "        # Find and visit all internal links\n",
    "        for a_tag in soup.find_all('a', href=True):\n",
    "            href = a_tag['href']\n",
    "\n",
    "            # Check if the link is relevant for our search\n",
    "            if not is_relevant_link(base_url, href):\n",
    "                continue\n",
    "\n",
    "            # Check if href starts with http or https\n",
    "            if href.startswith('http'):\n",
    "                find_emails_on_page(base_url, href, visited_urls, depth=depth+1)\n",
    "            else:\n",
    "                # Construct full URL for relative paths\n",
    "                new_url = urljoin(base_url, href)\n",
    "                find_emails_on_page(base_url, new_url, visited_urls, depth=depth+1)\n",
    "\n",
    "    except EmailFoundException as e:\n",
    "        raise  # Propagate the exception upwards\n",
    "    except Exception as e:\n",
    "        print(f\"Error while processing {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_website(url):\n",
    "    \"\"\"Process a website to find an email within a certain timeframe.\"\"\"\n",
    "\n",
    "    # Store the result of the email scraping in a list for thread-safe operations\n",
    "    result = []\n",
    "\n",
    "    def worker():\n",
    "        try:\n",
    "            email = find_emails_on_page(url, url, set())\n",
    "            if email:\n",
    "                result.append(email)\n",
    "        except EmailFoundException as e:\n",
    "            result.append(e.args[0])\n",
    "\n",
    "    # Create a thread for the email scraping process\n",
    "    thread = threading.Thread(target=worker)\n",
    "    thread.start()\n",
    "\n",
    "    # Wait for 7-8 seconds for the thread to complete\n",
    "    thread.join(10)  # You can adjust this value as needed\n",
    "\n",
    "    # If the thread is still alive after the timeout, it means it's still working and we can stop it and move on\n",
    "    if thread.is_alive():\n",
    "        print(f\"Timeout reached for {url}. Moving to next website.\")\n",
    "        return None  # or return a placeholder message if desired\n",
    "\n",
    "    return result[0] if result else None\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# emails_list = []\n",
    "for url in BASE_URLS:\n",
    "    email = process_website(url)\n",
    "    if email:\n",
    "        print(f\"Found email: {email} on {url}\")\n",
    "        emails_list.append(email)\n",
    "    else:\n",
    "        emails_list.append('None')\n",
    "\n",
    "print(\"Finished processing all websites.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffb8318",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_swiss_boutiques['emails'] = emails_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fc3441",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_swiss_boutiques.to_excel('swiss_boutiques_with_emails.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c105cdfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
